preprocess:
  dataset:
    __target__: gslm.preprocess.preprocess_dataset.GlobWavDataset
    roots: ["/mnt/hdd/datasets/VCTK-Corpus/wav48/"]
    patterns: ["**/*.wav"]
    shuffled: False
  speech_ssl:
    model_path: rinna/japanese-hubert-base
  kmeans:
    model_path: kmeans_centroids.npy
    cfg:
      k: 100
      niter: 1000
      verbose: True
      seed: 1234
      nredo: 1
  layer: 6
  feature_output_path: /mnt/hdd/GSLM/feature-%06d.tar.gz
ulm:
  tokenizer_path: gslm_tokenizer/tokenizer.json
  dataset:
    folder: /mnt/hdd/GSLM/
    pattern: extracted_feature.split_*
  output_json:
    train: /mnt/hdd/GSLM/train.jsonl
    valid: /mnt/hdd/GSLM/valid.jsonl
    test: /mnt/hdd/GSLM/test.jsonl
  num_data:
    valid: 1000
    test: 1000
u2s:
  datamodule:
    _target_: gslm.u2s.dataset.datamodule.u2sDataModule
    _recursive_: false
    cfg:
      sample_rate: 22_050
      train_dataset:
        _target_: gslm.u2s.dataset.fairseq_manifest_dataset.FairseqManifestDataset
        manifest_path: /mnt/hdd/GSLM/manifest.txt
        resampled_sr: 22_050
        extracted_feature_path: /mnt/hdd/GSLM/extracted_feature.txt
      val_dataset:
        _target_: gslm.u2s.dataset.fairseq_manifest_dataset.FairseqManifestDataset
        manifest_path: /mnt/hdd/GSLM/manifest.txt
        resampled_sr: 22_050
        extracted_feature_path: /mnt/hdd/GSLM/extracted_feature.txt
      data:
        segment_size:
          train: 50
          val: -1
        train_batch_size: 32
        val_batch_size: 1
        target_feature: 
          key: input_feature
          samples_per_sec: 50
          bias: 0.0025
  lightning_module:
    _target_: gslm.u2s.hifigan.lightning_module.HiFiGANEmbeddingLightningModule
    _recursive_: false
    cfg:
      sample_rate: 22_050
      preprocess:
        stft:
          n_fft: 1024
          win_length: 1024
          hop_length: 256
          power: 1
          center: true
        mel:
          n_mels: 80
          sample_rate: 22_050
          f_min: 0
          f_max: 8000
          n_stft: 513
          norm: "slaney"
          mel_scale: "slaney"
      model:
        generator:
          num_tokens: 100
          num_input_channels: 512
          upsample_rates: [7,7,3,3]
          upsample_initial_channel: 512
          upsample_kernel_sizes: [15,15,7,7]
          resblock_dilation_sizes: [[1,3,5], [1,3,5], [1,3,5]]
          resblock_kernel_sizes: [3,7,11]
          resblock: "1"

        optim:
          opt_g:
            _target_: torch.optim.AdamW
            lr: 0.0002
            betas: [0.8,0.99]
          opt_d:
            _target_: torch.optim.AdamW
            lr: 0.0002
            betas: [0.8,0.99]
          scheduler_g:
            _target_: torch.optim.lr_scheduler.ExponentialLR
            gamma: 0.999998
          scheduler_d:
            _target_: torch.optim.lr_scheduler.ExponentialLR
            gamma: 0.999998
        adversarial_start_step: 10_000

        loss:
          recons_coef: 45
          fm_mpd_coef: 1
          fm_msd_coef: 1
          g_mpd_coef: 1
          g_msd_coef: 1
        logging_wav_samples: 10
  trainer:
    _target_: lightning.Trainer
    accelerator: "gpu"
    devices: [0]
    precision: 32
    check_val_every_n_epoch: 1
    max_epochs: 3300
  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: "tb_logs"
    - _target_: lightning.pytorch.loggers.WandbLogger
      project: "u2s"
      log_model: "all"